{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec, Word2Vec, Doc2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, LabeledSentence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import jobs, applicants\n",
    "import topic_model, nlp_utils\n",
    "\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = topic_model.get_stop_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Vetorizer params\n",
    "min_df=20\n",
    "max_df=.25\n",
    "max_vocab_size=50000\n",
    "dim_size=500\n",
    "ngram_range=(1, 1)\n",
    "\n",
    "# Vectorizer to be used throughout\n",
    "vectorizer = TfidfVectorizer(token_pattern=topic_model.get_token_pattern(),\n",
    "                                              min_df=min_df,\n",
    "                                              max_df=max_df,\n",
    "                                              max_features=max_vocab_size,\n",
    "                                              stop_words=topic_model.get_stop_words(),\n",
    "                                              ngram_range=ngram_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting job posting data...\n",
      "- Time: 0.529675960541\n",
      "\n",
      "(4015, 6)\n"
     ]
    }
   ],
   "source": [
    "# Get job postings, vectorizer, token generator\n",
    "df_jobs = jobs.get_job_posting_data()\n",
    "df_jobs = df_jobs[20000:].reset_index()\n",
    "print df_jobs.shape\n",
    "docs = df_jobs.description\n",
    "\n",
    "## Dont need a token generator anymore if we get implement the doc2vec model \n",
    "## with calls to build vocab and train using labeled sentence tokens\n",
    "# docgen = topic_model.TokenGenerator_skl(vectorizer, docs, topic_model.get_stop_words())\n",
    "# docgen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        20000\n",
       "id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           13528\n",
       "business_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  11438\n",
       "title                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Men's Clothing Salesperson\n",
       "description    Jack's, one of San Francisco's top men's clothing stores that specializes in vintage sports tees, is looking for a part-time men's clothing salesperson to work at our North Beach store.\\n\\nThis is a great job for a student who has a couple of free days and wants chill place to study while they work.\\n\\nThis job is all about your personality. Retail clothing experience is not necessary, but is helpful. Someone with a food service background or any job where you can prove you were a success at...\n",
       "tag_names                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Retail\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 500)\n",
    "df_jobs.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters for doc2vec\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----\n",
    "# Doc2Vec\n",
    "# ----\n",
    "In word2vec, the parameter “continuous bag of words” (cbow) and “skip-gram” (sg); \n",
    "in the doc2vec architecture, the corresponding algorithms are “distributed memory” (dm) and “distributed bag of words” (dbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing docs...\n",
      "- Time: 5.44096589088\n",
      "\n",
      "Creating LabeledSentences...\n",
      "- Time: 0.018s.\n"
     ]
    }
   ],
   "source": [
    "# Tokenize our docs\n",
    "d2v_ut = nlp_utils.D2V_Utils(vectorizer)\n",
    "docs_as_tokens = d2v_ut.get_tokenized_docs(docs)\n",
    "# Label our doc tokens\n",
    "labeled_docs = d2v_ut.get_tagged_docs(docs_as_tokens, 'job_posting')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labeled_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-09 04:42:18,743 : INFO : collecting all words and their counts\n",
      "2018-01-09 04:42:18,745 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Doc2Vec model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-09 04:42:19,072 : INFO : PROGRESS: at example #10000, processed 1053349 words (3228063/s), 26006 word types, 10000 tags\n",
      "2018-01-09 04:42:19,382 : INFO : PROGRESS: at example #20000, processed 2076333 words (3315229/s), 36904 word types, 20000 tags\n",
      "2018-01-09 04:42:19,504 : INFO : collected 40557 word types and 24015 unique tags from a corpus of 24015 examples and 2464929 words\n",
      "2018-01-09 04:42:19,505 : INFO : Loading a fresh vocabulary\n",
      "2018-01-09 04:42:19,557 : INFO : min_count=5 retains 14061 unique words (34% of original 40557, drops 26496)\n",
      "2018-01-09 04:42:19,558 : INFO : min_count=5 leaves 2422403 word corpus (98% of original 2464929, drops 42526)\n",
      "2018-01-09 04:42:19,672 : INFO : deleting the raw counts dictionary of 40557 items\n",
      "2018-01-09 04:42:19,675 : INFO : sample=0.001 downsamples 11 most-common words\n",
      "2018-01-09 04:42:19,677 : INFO : downsampling leaves estimated 2402599 word corpus (99.2% of prior 2422403)\n",
      "2018-01-09 04:42:19,678 : INFO : estimated required memory for 14061 words and 300 dimensions: 74397900 bytes\n",
      "2018-01-09 04:42:19,723 : INFO : resetting layer weights\n",
      "2018-01-09 04:42:20,264 : INFO : training model with 4 workers on 14061 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-01-09 04:42:21,312 : INFO : PROGRESS: at 0.52% examples, 253995 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:22,340 : INFO : PROGRESS: at 1.06% examples, 260316 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:42:23,386 : INFO : PROGRESS: at 1.67% examples, 260628 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:24,448 : INFO : PROGRESS: at 2.17% examples, 264390 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:25,475 : INFO : PROGRESS: at 2.75% examples, 262921 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:26,489 : INFO : PROGRESS: at 3.34% examples, 264030 words/s, in_qsize 8, out_qsize 1\n",
      "2018-01-09 04:42:27,518 : INFO : PROGRESS: at 3.93% examples, 264272 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:28,550 : INFO : PROGRESS: at 4.50% examples, 264310 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:29,573 : INFO : PROGRESS: at 5.07% examples, 264643 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:42:30,581 : INFO : PROGRESS: at 5.63% examples, 265147 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:31,633 : INFO : PROGRESS: at 6.16% examples, 264763 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:32,701 : INFO : PROGRESS: at 6.77% examples, 264740 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:33,791 : INFO : PROGRESS: at 7.29% examples, 265775 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:42:34,801 : INFO : PROGRESS: at 7.89% examples, 266164 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:35,804 : INFO : PROGRESS: at 8.46% examples, 265943 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:42:36,819 : INFO : PROGRESS: at 9.03% examples, 266150 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:37,851 : INFO : PROGRESS: at 9.62% examples, 266062 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:38,867 : INFO : PROGRESS: at 10.15% examples, 265694 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:39,867 : INFO : PROGRESS: at 10.73% examples, 266102 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:42:40,888 : INFO : PROGRESS: at 11.31% examples, 266266 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:41,904 : INFO : PROGRESS: at 11.89% examples, 266370 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:42,923 : INFO : PROGRESS: at 12.35% examples, 267326 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:43,930 : INFO : PROGRESS: at 12.93% examples, 267113 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:42:44,943 : INFO : PROGRESS: at 13.52% examples, 267240 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:45,955 : INFO : PROGRESS: at 14.07% examples, 266959 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:42:46,977 : INFO : PROGRESS: at 14.67% examples, 266964 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:48,001 : INFO : PROGRESS: at 15.23% examples, 266947 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:49,023 : INFO : PROGRESS: at 15.80% examples, 266946 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:50,041 : INFO : PROGRESS: at 16.37% examples, 266696 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:51,045 : INFO : PROGRESS: at 16.96% examples, 267174 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:42:52,097 : INFO : PROGRESS: at 17.43% examples, 267553 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:42:53,111 : INFO : PROGRESS: at 18.01% examples, 267345 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:42:54,113 : INFO : PROGRESS: at 18.61% examples, 267791 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:55,117 : INFO : PROGRESS: at 19.19% examples, 267911 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:56,145 : INFO : PROGRESS: at 19.73% examples, 267355 words/s, in_qsize 7, out_qsize 1\n",
      "2018-01-09 04:42:57,162 : INFO : PROGRESS: at 20.30% examples, 267614 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:58,163 : INFO : PROGRESS: at 20.89% examples, 267768 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:42:59,198 : INFO : PROGRESS: at 21.46% examples, 267453 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:00,243 : INFO : PROGRESS: at 22.02% examples, 267529 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:01,258 : INFO : PROGRESS: at 22.52% examples, 267820 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:02,307 : INFO : PROGRESS: at 23.12% examples, 267646 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:03,312 : INFO : PROGRESS: at 23.70% examples, 267771 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:04,337 : INFO : PROGRESS: at 24.30% examples, 267943 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:05,369 : INFO : PROGRESS: at 24.86% examples, 267871 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:06,388 : INFO : PROGRESS: at 25.41% examples, 267868 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:07,414 : INFO : PROGRESS: at 25.99% examples, 268049 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:08,436 : INFO : PROGRESS: at 26.61% examples, 268036 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:09,452 : INFO : PROGRESS: at 27.09% examples, 268247 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:10,498 : INFO : PROGRESS: at 27.69% examples, 268315 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:11,554 : INFO : PROGRESS: at 28.30% examples, 268116 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:12,558 : INFO : PROGRESS: at 28.88% examples, 268195 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:13,573 : INFO : PROGRESS: at 29.44% examples, 268026 words/s, in_qsize 7, out_qsize 1\n",
      "2018-01-09 04:43:14,635 : INFO : PROGRESS: at 30.01% examples, 267996 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:15,644 : INFO : PROGRESS: at 30.56% examples, 268056 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:16,705 : INFO : PROGRESS: at 31.15% examples, 268234 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:17,789 : INFO : PROGRESS: at 31.77% examples, 268100 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:18,814 : INFO : PROGRESS: at 32.28% examples, 268574 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:19,887 : INFO : PROGRESS: at 32.89% examples, 268490 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:20,900 : INFO : PROGRESS: at 33.48% examples, 268525 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:21,937 : INFO : PROGRESS: at 34.05% examples, 268435 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:22,979 : INFO : PROGRESS: at 34.67% examples, 268487 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:24,042 : INFO : PROGRESS: at 35.25% examples, 268444 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:25,098 : INFO : PROGRESS: at 35.83% examples, 268431 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:26,109 : INFO : PROGRESS: at 36.45% examples, 268617 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:27,143 : INFO : PROGRESS: at 37.00% examples, 268689 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:28,165 : INFO : PROGRESS: at 37.51% examples, 268821 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:29,196 : INFO : PROGRESS: at 38.11% examples, 268769 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:30,246 : INFO : PROGRESS: at 38.68% examples, 268644 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-09 04:43:31,260 : INFO : PROGRESS: at 39.27% examples, 268658 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:32,283 : INFO : PROGRESS: at 39.82% examples, 268626 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:33,305 : INFO : PROGRESS: at 40.40% examples, 268747 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:34,306 : INFO : PROGRESS: at 40.97% examples, 268821 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:35,344 : INFO : PROGRESS: at 41.57% examples, 268630 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:36,358 : INFO : PROGRESS: at 42.09% examples, 268883 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:37,417 : INFO : PROGRESS: at 42.66% examples, 268870 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:38,427 : INFO : PROGRESS: at 43.26% examples, 268896 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:39,456 : INFO : PROGRESS: at 43.87% examples, 268978 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:40,465 : INFO : PROGRESS: at 44.45% examples, 268996 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:41,522 : INFO : PROGRESS: at 45.04% examples, 268976 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:42,557 : INFO : PROGRESS: at 45.60% examples, 269025 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:43,672 : INFO : PROGRESS: at 46.19% examples, 268952 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:44,675 : INFO : PROGRESS: at 46.77% examples, 268989 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:45,686 : INFO : PROGRESS: at 47.27% examples, 269252 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:46,701 : INFO : PROGRESS: at 47.86% examples, 269244 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:47,724 : INFO : PROGRESS: at 48.47% examples, 269331 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:48,767 : INFO : PROGRESS: at 49.05% examples, 269249 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:49,845 : INFO : PROGRESS: at 49.68% examples, 269274 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:50,848 : INFO : PROGRESS: at 50.24% examples, 269307 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:51,867 : INFO : PROGRESS: at 50.83% examples, 269408 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:52,904 : INFO : PROGRESS: at 51.42% examples, 269359 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:53,914 : INFO : PROGRESS: at 51.98% examples, 269366 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:43:54,922 : INFO : PROGRESS: at 52.45% examples, 269492 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:55,968 : INFO : PROGRESS: at 53.08% examples, 269506 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:57,031 : INFO : PROGRESS: at 53.65% examples, 269381 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:58,037 : INFO : PROGRESS: at 54.23% examples, 269394 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:43:59,041 : INFO : PROGRESS: at 54.82% examples, 269520 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:00,081 : INFO : PROGRESS: at 55.39% examples, 269545 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:01,114 : INFO : PROGRESS: at 55.97% examples, 269630 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:02,144 : INFO : PROGRESS: at 56.59% examples, 269551 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:03,145 : INFO : PROGRESS: at 57.09% examples, 269676 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:04,147 : INFO : PROGRESS: at 57.63% examples, 269620 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:05,149 : INFO : PROGRESS: at 58.23% examples, 269649 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:06,178 : INFO : PROGRESS: at 58.84% examples, 269704 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:07,190 : INFO : PROGRESS: at 59.42% examples, 269705 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:08,206 : INFO : PROGRESS: at 59.97% examples, 269701 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:09,272 : INFO : PROGRESS: at 60.52% examples, 269570 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:10,286 : INFO : PROGRESS: at 61.08% examples, 269674 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:11,300 : INFO : PROGRESS: at 61.69% examples, 269673 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:12,369 : INFO : PROGRESS: at 62.18% examples, 269714 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:13,372 : INFO : PROGRESS: at 62.79% examples, 269740 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:14,418 : INFO : PROGRESS: at 63.40% examples, 269750 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:15,473 : INFO : PROGRESS: at 64.00% examples, 269748 words/s, in_qsize 7, out_qsize 1\n",
      "2018-01-09 04:44:16,469 : INFO : PROGRESS: at 64.58% examples, 269775 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:17,510 : INFO : PROGRESS: at 65.15% examples, 269793 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:18,523 : INFO : PROGRESS: at 65.72% examples, 269787 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:19,563 : INFO : PROGRESS: at 66.32% examples, 269815 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:20,572 : INFO : PROGRESS: at 66.93% examples, 269898 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:21,600 : INFO : PROGRESS: at 67.36% examples, 269946 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:22,626 : INFO : PROGRESS: at 68.01% examples, 270084 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:23,633 : INFO : PROGRESS: at 68.57% examples, 270016 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:24,704 : INFO : PROGRESS: at 69.19% examples, 270042 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:25,743 : INFO : PROGRESS: at 69.78% examples, 270060 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:26,752 : INFO : PROGRESS: at 70.36% examples, 270152 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:27,773 : INFO : PROGRESS: at 70.93% examples, 270136 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:28,813 : INFO : PROGRESS: at 71.54% examples, 270093 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:29,837 : INFO : PROGRESS: at 72.07% examples, 270130 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:30,843 : INFO : PROGRESS: at 72.60% examples, 270143 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:31,851 : INFO : PROGRESS: at 73.20% examples, 270154 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:32,854 : INFO : PROGRESS: at 73.78% examples, 270175 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:33,862 : INFO : PROGRESS: at 74.36% examples, 270178 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:34,888 : INFO : PROGRESS: at 74.94% examples, 270220 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:35,895 : INFO : PROGRESS: at 75.51% examples, 270297 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:36,909 : INFO : PROGRESS: at 76.07% examples, 270368 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:37,926 : INFO : PROGRESS: at 76.68% examples, 270356 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:38,978 : INFO : PROGRESS: at 77.20% examples, 270483 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:40,038 : INFO : PROGRESS: at 77.82% examples, 270462 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:41,046 : INFO : PROGRESS: at 78.42% examples, 270537 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:42,103 : INFO : PROGRESS: at 79.02% examples, 270520 words/s, in_qsize 7, out_qsize 1\n",
      "2018-01-09 04:44:43,107 : INFO : PROGRESS: at 79.62% examples, 270597 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:44,119 : INFO : PROGRESS: at 80.17% examples, 270592 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:45,135 : INFO : PROGRESS: at 80.75% examples, 270584 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:46,146 : INFO : PROGRESS: at 81.36% examples, 270661 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:47,153 : INFO : PROGRESS: at 81.96% examples, 270728 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:48,195 : INFO : PROGRESS: at 82.42% examples, 270802 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:49,215 : INFO : PROGRESS: at 83.03% examples, 270789 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:50,264 : INFO : PROGRESS: at 83.65% examples, 270850 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:51,295 : INFO : PROGRESS: at 84.25% examples, 270868 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:52,346 : INFO : PROGRESS: at 84.84% examples, 270858 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-09 04:44:53,353 : INFO : PROGRESS: at 85.41% examples, 270923 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:54,357 : INFO : PROGRESS: at 85.99% examples, 270997 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:55,396 : INFO : PROGRESS: at 86.61% examples, 270945 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:56,432 : INFO : PROGRESS: at 87.12% examples, 271081 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:57,462 : INFO : PROGRESS: at 87.73% examples, 271111 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:44:58,497 : INFO : PROGRESS: at 88.36% examples, 271136 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:44:59,530 : INFO : PROGRESS: at 88.94% examples, 271085 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:45:00,566 : INFO : PROGRESS: at 89.51% examples, 271037 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:45:01,578 : INFO : PROGRESS: at 90.09% examples, 271093 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:45:02,582 : INFO : PROGRESS: at 90.64% examples, 271040 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:45:03,628 : INFO : PROGRESS: at 91.20% examples, 271047 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:45:04,631 : INFO : PROGRESS: at 91.80% examples, 271055 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:45:05,668 : INFO : PROGRESS: at 92.28% examples, 271124 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:45:06,690 : INFO : PROGRESS: at 92.89% examples, 271163 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:45:07,711 : INFO : PROGRESS: at 93.48% examples, 271146 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:45:08,738 : INFO : PROGRESS: at 94.06% examples, 271111 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:45:09,777 : INFO : PROGRESS: at 94.67% examples, 271130 words/s, in_qsize 7, out_qsize 1\n",
      "2018-01-09 04:45:10,795 : INFO : PROGRESS: at 95.25% examples, 271162 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:45:11,832 : INFO : PROGRESS: at 95.84% examples, 271168 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:45:12,884 : INFO : PROGRESS: at 96.45% examples, 271177 words/s, in_qsize 8, out_qsize 1\n",
      "2018-01-09 04:45:13,934 : INFO : PROGRESS: at 97.02% examples, 271204 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:45:14,970 : INFO : PROGRESS: at 97.56% examples, 271274 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:45:15,992 : INFO : PROGRESS: at 98.13% examples, 271203 words/s, in_qsize 7, out_qsize 1\n",
      "2018-01-09 04:45:16,989 : INFO : PROGRESS: at 98.74% examples, 271270 words/s, in_qsize 8, out_qsize 0\n",
      "2018-01-09 04:45:18,002 : INFO : PROGRESS: at 99.32% examples, 271259 words/s, in_qsize 7, out_qsize 0\n",
      "2018-01-09 04:45:19,028 : INFO : PROGRESS: at 99.89% examples, 271285 words/s, in_qsize 6, out_qsize 0\n",
      "2018-01-09 04:45:19,087 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-09 04:45:19,098 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-09 04:45:19,113 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-09 04:45:19,133 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-09 04:45:19,134 : INFO : training on 49298580 raw words (48533369 effective words) took 178.9s, 271348 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Time: 178.872s.\n"
     ]
    }
   ],
   "source": [
    "print \"\\nCreating Doc2Vec model...\\n\"\n",
    "t1 = time.time()\n",
    "d2v_model = Doc2Vec(size=300, \n",
    "                    dbow_words= 1, \n",
    "                    window=10,\n",
    "                    min_count=5, \n",
    "                    workers=4)\n",
    "d2v_model.build_vocab(labeled_docs)\n",
    "\n",
    "t1 = time.time()\n",
    "d2v_model.train(labeled_docs, \n",
    "                total_examples=len(labeled_docs),\n",
    "                epochs=20)\n",
    "print \"- Time: %0.3fs.\" % (time.time() - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-09 04:45:19,147 : INFO : precomputing L2-norms of doc weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('job_posting_6917', 0.7508301734924316),\n",
       " ('job_posting_13270', 0.7499496340751648),\n",
       " ('job_posting_12357', 0.7456464767456055),\n",
       " ('job_posting_5852', 0.7208945155143738),\n",
       " ('job_posting_1962', 0.7184603214263916),\n",
       " ('job_posting_12246', 0.716362714767456),\n",
       " ('job_posting_17451', 0.714232325553894),\n",
       " ('job_posting_11426', 0.7122789621353149),\n",
       " ('job_posting_8534', 0.7092502117156982),\n",
       " ('job_posting_8414', 0.7085880041122437)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_str = 'job_posting_10'\n",
    "docsim = d2v_model.docvecs.most_similar(id_str)\n",
    "docsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2018-01-09 03:34:06,193 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'mediterranean', 0.514112114906311),\n",
       " (u'steak', 0.5054657459259033),\n",
       " (u'michelin', 0.5049415826797485),\n",
       " (u'fish', 0.5048666000366211),\n",
       " (u'florentine', 0.5015060305595398),\n",
       " (u'opentable', 0.4838712513446808),\n",
       " (u'bistro', 0.4608152210712433),\n",
       " (u'pasta', 0.4505152702331543),\n",
       " (u'sauce', 0.4236881136894226),\n",
       " (u'cocktail', 0.41957467794418335)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.similar_by_word('chef')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applicant Recommender\n",
    "\n",
    "## Applicant resume data query on jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting user and applicant data...\n",
      "- Time: 4.78302788734\n",
      "\n",
      "Processing user resume data...\n",
      "Tokenize failed for user id 16408, error: Failed to parse QName 'http:', line 675, column 179 (line 675)\n",
      "- Time: 50.5367798805\n",
      "\n",
      "Done.\n",
      "(183872, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>first_name</th>\n",
       "      <th>last_name</th>\n",
       "      <th>parsed_resume_xml</th>\n",
       "      <th>passions</th>\n",
       "      <th>introduction</th>\n",
       "      <th>job_tag_id</th>\n",
       "      <th>tag_type</th>\n",
       "      <th>resume_elements</th>\n",
       "      <th>res_executiveSummary</th>\n",
       "      <th>res_description</th>\n",
       "      <th>res_title</th>\n",
       "      <th>res_competency</th>\n",
       "      <th>res_degree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87468</td>\n",
       "      <td>Angelica</td>\n",
       "      <td>Espinoza</td>\n",
       "      <td>&lt;Resume xmlns=\"http://ns.hr-xml.org/2006-02-28...</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{u'Competency': u'ANSWERING, CASH, CASHIER, CA...</td>\n",
       "      <td>None</td>\n",
       "      <td>Massage Therapist/Receptionist\\n* 30,60 and 80...</td>\n",
       "      <td>None</td>\n",
       "      <td>ANSWERING, CASH, CASHIER, CASHIERING, CREDIT, ...</td>\n",
       "      <td>certification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88235</td>\n",
       "      <td>Julie</td>\n",
       "      <td>Nguyen</td>\n",
       "      <td>&lt;Resume xmlns=\"http://ns.hr-xml.org/2006-02-28...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{u'Competency': u'BILLING, BIOLOGICAL SCIENCES...</td>\n",
       "      <td>None</td>\n",
       "      <td>- Work on an interdisciplinary team to provide...</td>\n",
       "      <td>None</td>\n",
       "      <td>BILLING, BIOLOGICAL SCIENCES, LIAISON, NEUROSC...</td>\n",
       "      <td>bachelors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88237</td>\n",
       "      <td>Aparna</td>\n",
       "      <td>Upadhyaya</td>\n",
       "      <td>&lt;Resume xmlns=\"http://ns.hr-xml.org/2006-02-28...</td>\n",
       "      <td>[Cooking,  travel, dance,  yoga, pets]</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{u'Competency': u'APEX, BENEFITS, CAM, CATHETE...</td>\n",
       "      <td>None</td>\n",
       "      <td>● Assisting in providing a comprehensive plan ...</td>\n",
       "      <td>None</td>\n",
       "      <td>APEX, BENEFITS, CAM, CATHETER, CATHETERS, CHEM...</td>\n",
       "      <td>bachelors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>88249</td>\n",
       "      <td>Janet</td>\n",
       "      <td>Donohue</td>\n",
       "      <td>&lt;Resume xmlns=\"http://ns.hr-xml.org/2006-02-28...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{u'Competency': u'A/R, ACCOUNT RECONCILIATION,...</td>\n",
       "      <td>None</td>\n",
       "      <td>* Monitor and review credit limit and payment ...</td>\n",
       "      <td>None</td>\n",
       "      <td>A/R, ACCOUNT RECONCILIATION, ACCOUNT RECONCILI...</td>\n",
       "      <td>bachelors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>150252</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>I am a hard worker like to learn,teachable,alw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id first_name  last_name  \\\n",
       "0   87468   Angelica   Espinoza   \n",
       "1   88235      Julie     Nguyen   \n",
       "2   88237     Aparna  Upadhyaya   \n",
       "3   88249      Janet    Donohue   \n",
       "4  150252       None       None   \n",
       "\n",
       "                                   parsed_resume_xml  \\\n",
       "0  <Resume xmlns=\"http://ns.hr-xml.org/2006-02-28...   \n",
       "1  <Resume xmlns=\"http://ns.hr-xml.org/2006-02-28...   \n",
       "2  <Resume xmlns=\"http://ns.hr-xml.org/2006-02-28...   \n",
       "3  <Resume xmlns=\"http://ns.hr-xml.org/2006-02-28...   \n",
       "4                                               None   \n",
       "\n",
       "                                 passions  \\\n",
       "0                                      []   \n",
       "1                                    None   \n",
       "2  [Cooking,  travel, dance,  yoga, pets]   \n",
       "3                                    None   \n",
       "4                                      []   \n",
       "\n",
       "                                        introduction  job_tag_id  tag_type  \\\n",
       "0                                               None         NaN       NaN   \n",
       "1                                               None         NaN       NaN   \n",
       "2                                               None         NaN       NaN   \n",
       "3                                               None         NaN       NaN   \n",
       "4  I am a hard worker like to learn,teachable,alw...         NaN       NaN   \n",
       "\n",
       "                                     resume_elements res_executiveSummary  \\\n",
       "0  {u'Competency': u'ANSWERING, CASH, CASHIER, CA...                 None   \n",
       "1  {u'Competency': u'BILLING, BIOLOGICAL SCIENCES...                 None   \n",
       "2  {u'Competency': u'APEX, BENEFITS, CAM, CATHETE...                 None   \n",
       "3  {u'Competency': u'A/R, ACCOUNT RECONCILIATION,...                 None   \n",
       "4                                                 {}                 None   \n",
       "\n",
       "                                     res_description res_title  \\\n",
       "0  Massage Therapist/Receptionist\\n* 30,60 and 80...      None   \n",
       "1  - Work on an interdisciplinary team to provide...      None   \n",
       "2  ● Assisting in providing a comprehensive plan ...      None   \n",
       "3  * Monitor and review credit limit and payment ...      None   \n",
       "4                                               None      None   \n",
       "\n",
       "                                      res_competency     res_degree  \n",
       "0  ANSWERING, CASH, CASHIER, CASHIERING, CREDIT, ...  certification  \n",
       "1  BILLING, BIOLOGICAL SCIENCES, LIAISON, NEUROSC...      bachelors  \n",
       "2  APEX, BENEFITS, CAM, CATHETER, CATHETERS, CHEM...      bachelors  \n",
       "3  A/R, ACCOUNT RECONCILIATION, ACCOUNT RECONCILI...      bachelors  \n",
       "4                                               None           None  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get applicant data, vectorizer, token generator\n",
    "df_appl = applicants.get_applicant_data()\n",
    "print df_appl.shape\n",
    "\n",
    "df_appl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                                                                  88235\n",
      "first_name                                                          Julie\n",
      "last_name                                                          Nguyen\n",
      "parsed_resume_xml       <Resume xmlns=\"http://ns.hr-xml.org/2006-02-28...\n",
      "passions                                                             None\n",
      "introduction                                                         None\n",
      "job_tag_id                                                            NaN\n",
      "tag_type                                                              NaN\n",
      "resume_elements         {u'Competency': u'BILLING, BIOLOGICAL SCIENCES...\n",
      "res_executiveSummary                                                 None\n",
      "res_description         - Work on an interdisciplinary team to provide...\n",
      "res_title                                                            None\n",
      "res_competency          BILLING, BIOLOGICAL SCIENCES, LIAISON, NEUROSC...\n",
      "res_degree                                                      bachelors\n",
      "Name: 1, dtype: object\n",
      "- Work on an interdisciplinary team to provide parent education and direct therapy for young children with developmental delays\n",
      "- Perform assessments using BDI-2 and REEL-3 protocols, then: determining areas of concern, writing individualized goals, and submitting reports and service recommendations to the regional center\n",
      "- Conduct Individualized Family Service Plan, transitional, and exit meetings with the families, regional service coordinator, and school districts\n",
      "- Manage a caseload of 25 clients and handle all scheduling matters, - Administered direct ABA therapeutic interventions to children with autism spectrum disorder to increase skills in areas of concern and address challenging behaviors\n",
      "- Recorded data and kept detailed notes of child's progress\n",
      "- Communicated with program supervisors, board certified behavior analysts, and parents in weekly clinical meetings to create comprehensive goals and behavioral support plans\n",
      "PROFESSIONAL EXPERIENCE CONTINUED, - Promoted patient satisfaction by communicating patient needs to the Unit Manager and assisting in addressing these needs\n",
      "- Made post-discharge calls through Patient Call Manager to ensure the patient's smooth and safe transition home, - Acted as a liaison for patients leading up to and following their appointments\n",
      "- Job Responsibilities included: scheduling appointments, authorizing insurance/billing information, helping patients complete required forms, performing vision pre-tests, and updating patients on the status of their orders, - Participated in the foundation's Summer Medical Mission 2012. The camp served over 3,500 patients in five impoverished, rural areas in Vietnam.\n",
      "- Departments: gynecology, internal medicine, pediatrics, dental, nutrition counseling, and vitals, JULIE NGUYEN's experience appears to be concentrated in Nursing / Billing and Coding, with exposure to Clinical / Tests and Functions. JULIE NGUYEN has 5 years of work experience, with little or no management experience.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "u = df_appl.iloc[1]\n",
    "print u\n",
    "\n",
    "u_res_desc = u.res_description\n",
    "print u_res_desc\n",
    "\n",
    "# convert a user resume to a doc2vec TaggedDocument to run on the jobs model\n",
    "words = u_res_desc.split()\n",
    "proc_words = [lemmer.lemmatize(w) for w in list(set(analyzer(doc)) - set(sw)) if pattern.match(w)]\n",
    "labeled_doc = LabeledSentence(words=proc_words, tags=['user_1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SENT_6309', 0.5231666564941406),\n",
       " ('SENT_2860', 0.4562077522277832),\n",
       " ('SENT_19929', 0.42022114992141724),\n",
       " ('SENT_10245', 0.36824649572372437),\n",
       " ('SENT_17375', 0.3603281080722809),\n",
       " ('SENT_9134', 0.35061541199684143),\n",
       " ('SENT_7316', 0.3451305627822876),\n",
       " ('SENT_16119', 0.34335872530937195),\n",
       " ('SENT_15130', 0.3404138684272766),\n",
       " ('SENT_2244', 0.3388136923313141)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d2v_model.similar_by_word(u)\n",
    "tokens = \"Massage Therapist esthetician Receptionist store greeter\".split()\n",
    "# tokens = proc_words\n",
    "\n",
    "new_vector = d2v_model.infer_vector(tokens)\n",
    "#new_vector = d2v_model.infer_vector(labeled_doc)\n",
    "sims = d2v_model.docvecs.most_similar([new_vector]) #gives you top 10 document tags and their cosine similarity\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similar_by_word` (Method will be removed in 4.0.0, use self.wv.similar_by_word() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'esthetician', 0.6112874150276184),\n",
       " (u'lash', 0.5931062698364258),\n",
       " (u'cosmetologist', 0.5857604146003723),\n",
       " (u'camtc', 0.5329710841178894),\n",
       " (u'mani', 0.521687924861908),\n",
       " (u'eyebrow', 0.5181446671485901),\n",
       " (u'facial', 0.514720618724823),\n",
       " (u'manicure', 0.5129488706588745),\n",
       " (u'grooming', 0.5102207660675049),\n",
       " (u'complimentary', 0.49629977345466614)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2v_model.similar_by_word('massage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Helen Miller Bridal flagship store located in San Francisco is looking for a Bridal Stylist/Sales to join our team.  \\nHere, at Helen Miller Bridal, we strive to make each experience with every bride one to remember. Customer Service & Sales Experience are not required but greatly appreciated. This is a part-time position starting out and mostly on weekends, but has the potential to grow into a full-time position.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs.iloc[19929].description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applicant to applicant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing docs\n",
      "- Time: 14.8597640991\n",
      "\n",
      "Creating LabeledSentences...\n",
      "- Time: 0.302s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Preprocess the appicant resumes as labeled doc tokens\n",
    "docs = df_appl.res_description\n",
    "docs_as_tokens = get_tokenized_docs(docs)\n",
    "labeled_docs = get_labeled_sentences(docs_as_tokens, 'appl_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=[u'operation', u'material', u'managed', u'office', u'money', u'organized', u'year', u'drawer', u'therapist', u'receptionist', u'entering', u'manner', u'counting', u'technology', u'exam', u'angelica', u'information', u'clerical', u'monitoring', u'answering', u'mid', u'establishment', u'payment', u'form', u'going', u'minute', u'objective', u'consultation', u'administrative', u'correct', u'booking', u'greeting', u'file', u'timely', u'customer', u'patient', u'complete', u'match', u'completed', u'enter', u'paper', u'phone', u'appointment', u'management', u'determination', u'documented', u'beginning', u'appears', u'change', u'exposure', u'concentrated', u'massage', u'lower', u'debit', u'room', u'call', u'level', u'receive', u'client', u'message', u'cash', u'espinoza', u'specific', u'credit', u'filling', u'contact', u'ensure', u'guiding', u'contain', u'card', u'did', u'automatic', u'massage', u'software'], tags=['appl__0'])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print len(labeled_docs)\n",
    "labeled_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-09 04:22:15,400 : INFO : collecting all words and their counts\n",
      "2018-01-09 04:22:15,405 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Doc2Vec model...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-09 04:22:15,780 : INFO : PROGRESS: at example #10000, processed 1210693 words (3252550/s), 49509 word types, 10000 tags\n",
      "2018-01-09 04:22:15,924 : INFO : collected 59845 word types and 13869 unique tags from a corpus of 13869 examples and 1672789 words\n",
      "2018-01-09 04:22:15,925 : INFO : Loading a fresh vocabulary\n",
      "2018-01-09 04:22:15,983 : INFO : min_count=5 retains 13722 unique words (22% of original 59845, drops 46123)\n",
      "2018-01-09 04:22:15,984 : INFO : min_count=5 leaves 1608114 word corpus (96% of original 1672789, drops 64675)\n",
      "2018-01-09 04:22:16,023 : INFO : deleting the raw counts dictionary of 59845 items\n",
      "2018-01-09 04:22:16,027 : INFO : sample=0.001 downsamples 22 most-common words\n",
      "2018-01-09 04:22:16,030 : INFO : downsampling leaves estimated 1551943 word corpus (96.5% of prior 1608114)\n",
      "2018-01-09 04:22:16,031 : INFO : estimated required memory for 13722 words and 300 dimensions: 59210400 bytes\n",
      "2018-01-09 04:22:16,074 : INFO : resetting layer weights\n",
      "2018-01-09 04:22:16,460 : INFO : training model with 4 workers on 13722 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n"
     ]
    }
   ],
   "source": [
    "print \"\\nCreating Doc2Vec model...\\n\"\n",
    "t1 = time.time()\n",
    "d2v_appl_model = Doc2Vec(size=300, \n",
    "                    dbow_words= 1, \n",
    "                    window=10,\n",
    "                    min_count=5, \n",
    "                    workers=4)\n",
    "d2v_appl_model.build_vocab(labeled_docs)\n",
    "print \"- Time: %0.3fs.\" % (time.time() - t1)\n",
    "\n",
    "print \"\\nCreating Doc2Vec model...\\n\"\n",
    "t1 = time.time()\n",
    "d2v_model.train(labeled_docs, \n",
    "                total_examples=len(labeled_docs),\n",
    "                epochs=20)\n",
    "print \"- Time: %0.3fs.\" % (time.time() - t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = df_appl.description[0:20000]\n",
    "docs = df_appl.description\n",
    "\n",
    "docgen = topic_model.TokenGenerator_skl(vectorizer, docs, topic_model.get_stop_words())\n",
    "docgen\n",
    "type(docgen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Interesting:\n",
    "# Check that the document vector after training is different than the one before training. \n",
    "\n",
    "# prepare documents by TaggedDocument\n",
    "# docs = ...\n",
    "\n",
    "# an article tag\n",
    "id_str = 'SENT_3'\n",
    "\n",
    "# initialize a model\n",
    "model = Doc2Vec(size=300, window=20, min_count=2, workers=8, alpha=0.025, min_alpha=0.01, dm=0)\n",
    "\n",
    "# build vocabulary\n",
    "model.build_vocab(labeled_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docvec1: [ -5.78184205e-04  -1.61719881e-03   1.56266044e-03  -2.00020822e-05\n",
      "   7.59142451e-04   5.63495560e-04  -3.32428259e-04  -9.13201016e-04\n",
      "  -1.43482396e-03   9.33754782e-04  -1.60543714e-03   6.80618919e-04\n",
      "  -1.38343510e-03  -1.50158152e-03   1.46248657e-03  -1.34379754e-03\n",
      "   6.69001078e-04  -9.09951050e-04  -1.17913017e-03   1.42662937e-03]\n",
      "docvecsyn1: [ -5.78184205e-04  -1.61719881e-03   1.56266044e-03  -2.00020822e-05\n",
      "   7.59142451e-04   5.63495560e-04  -3.32428259e-04  -9.13201016e-04\n",
      "  -1.43482396e-03   9.33754782e-04  -1.60543714e-03   6.80618919e-04\n",
      "  -1.38343510e-03  -1.50158152e-03   1.46248657e-03  -1.34379754e-03\n",
      "   6.69001078e-04  -9.09951050e-04  -1.17913017e-03   1.42662937e-03]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('SENT_22468', 0.2260284423828125),\n",
       " ('SENT_16903', 0.21625030040740967),\n",
       " ('SENT_13529', 0.2105707973241806),\n",
       " ('SENT_9124', 0.21006619930267334),\n",
       " ('SENT_23702', 0.20848970115184784),\n",
       " ('SENT_2737', 0.2053980678319931),\n",
       " ('SENT_6350', 0.2043028175830841),\n",
       " ('SENT_9955', 0.2029171884059906),\n",
       " ('SENT_13556', 0.19606398046016693),\n",
       " ('SENT_13329', 0.19407835602760315)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the initial document vector\n",
    "docvec1 = model.docvecs[0]\n",
    "docvecsyn1 = model.docvecs.doctag_syn0[0]\n",
    "\n",
    "# calculate most similar documents\n",
    "# (the model is not trained, so the results should be wrong)\n",
    "docsim1 = model.docvecs.most_similar(id_str)\n",
    "\n",
    "print 'docvec1: %s' % docvec1[:20]\n",
    "print 'docvecsyn1: %s' % docvecsyn1[:20]\n",
    "docsim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-08 06:16:08,169 : INFO : training model with 8 workers on 23856 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=20\n",
      "2018-01-08 06:16:09,194 : INFO : PROGRESS: at 0.70% examples, 342635 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:10,253 : INFO : PROGRESS: at 1.62% examples, 381370 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:11,263 : INFO : PROGRESS: at 2.42% examples, 403432 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:12,286 : INFO : PROGRESS: at 3.38% examples, 408861 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:13,289 : INFO : PROGRESS: at 4.28% examples, 413580 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:14,291 : INFO : PROGRESS: at 5.19% examples, 418564 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:15,301 : INFO : PROGRESS: at 6.04% examples, 420089 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:16,325 : INFO : PROGRESS: at 6.98% examples, 421666 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:17,334 : INFO : PROGRESS: at 7.80% examples, 421941 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:18,327 : INFO : PROGRESS: at 8.74% examples, 424134 words/s, in_qsize 14, out_qsize 0\n",
      "2018-01-08 06:16:19,362 : INFO : PROGRESS: at 9.66% examples, 424538 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:20,366 : INFO : PROGRESS: at 10.57% examples, 426040 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:21,386 : INFO : PROGRESS: at 11.46% examples, 425395 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:22,401 : INFO : PROGRESS: at 12.28% examples, 427662 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:23,411 : INFO : PROGRESS: at 13.25% examples, 427787 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:24,445 : INFO : PROGRESS: at 14.14% examples, 427876 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:25,450 : INFO : PROGRESS: at 15.08% examples, 428700 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:26,466 : INFO : PROGRESS: at 15.96% examples, 429158 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:27,498 : INFO : PROGRESS: at 16.88% examples, 428208 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:28,507 : INFO : PROGRESS: at 17.76% examples, 430250 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:29,534 : INFO : PROGRESS: at 18.66% examples, 429928 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:30,537 : INFO : PROGRESS: at 19.58% examples, 430035 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:31,620 : INFO : PROGRESS: at 20.52% examples, 429981 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:32,644 : INFO : PROGRESS: at 21.46% examples, 430228 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:33,682 : INFO : PROGRESS: at 22.28% examples, 430836 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:34,724 : INFO : PROGRESS: at 23.25% examples, 430315 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:35,734 : INFO : PROGRESS: at 24.18% examples, 431339 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:36,806 : INFO : PROGRESS: at 25.10% examples, 430379 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:37,850 : INFO : PROGRESS: at 26.02% examples, 431182 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:38,868 : INFO : PROGRESS: at 27.00% examples, 431673 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:39,916 : INFO : PROGRESS: at 27.82% examples, 431105 words/s, in_qsize 16, out_qsize 1\n",
      "2018-01-08 06:16:40,919 : INFO : PROGRESS: at 28.79% examples, 431773 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:41,946 : INFO : PROGRESS: at 29.65% examples, 431260 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:42,942 : INFO : PROGRESS: at 30.56% examples, 431615 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:43,993 : INFO : PROGRESS: at 31.48% examples, 431120 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:45,013 : INFO : PROGRESS: at 32.32% examples, 432018 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:46,018 : INFO : PROGRESS: at 33.29% examples, 432066 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:47,043 : INFO : PROGRESS: at 34.18% examples, 432105 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:48,053 : INFO : PROGRESS: at 35.10% examples, 432017 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:49,105 : INFO : PROGRESS: at 35.99% examples, 431994 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:16:50,110 : INFO : PROGRESS: at 36.97% examples, 432462 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:51,132 : INFO : PROGRESS: at 37.80% examples, 432519 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:52,147 : INFO : PROGRESS: at 38.66% examples, 431965 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:53,216 : INFO : PROGRESS: at 39.60% examples, 431573 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:54,257 : INFO : PROGRESS: at 40.52% examples, 431675 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:55,286 : INFO : PROGRESS: at 41.46% examples, 431703 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:56,314 : INFO : PROGRESS: at 42.26% examples, 431900 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:57,319 : INFO : PROGRESS: at 43.17% examples, 431515 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:58,335 : INFO : PROGRESS: at 44.06% examples, 431429 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:16:59,340 : INFO : PROGRESS: at 45.01% examples, 431633 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:00,350 : INFO : PROGRESS: at 45.87% examples, 431596 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:01,372 : INFO : PROGRESS: at 46.78% examples, 431467 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:02,418 : INFO : PROGRESS: at 47.62% examples, 431516 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:03,424 : INFO : PROGRESS: at 48.55% examples, 431703 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:04,458 : INFO : PROGRESS: at 49.49% examples, 431517 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:05,457 : INFO : PROGRESS: at 50.33% examples, 431545 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:06,518 : INFO : PROGRESS: at 51.25% examples, 431164 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:07,549 : INFO : PROGRESS: at 52.12% examples, 431532 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:08,543 : INFO : PROGRESS: at 53.02% examples, 431567 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:09,569 : INFO : PROGRESS: at 53.99% examples, 431600 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:10,572 : INFO : PROGRESS: at 54.83% examples, 431452 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:11,610 : INFO : PROGRESS: at 55.74% examples, 431228 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:12,642 : INFO : PROGRESS: at 56.68% examples, 431374 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:13,652 : INFO : PROGRESS: at 57.46% examples, 431339 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:14,670 : INFO : PROGRESS: at 58.42% examples, 431427 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:15,681 : INFO : PROGRESS: at 59.32% examples, 431250 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:16,686 : INFO : PROGRESS: at 60.19% examples, 431265 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:17,696 : INFO : PROGRESS: at 61.08% examples, 431410 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:18,718 : INFO : PROGRESS: at 62.02% examples, 431450 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:19,724 : INFO : PROGRESS: at 62.86% examples, 431592 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:20,726 : INFO : PROGRESS: at 63.78% examples, 431493 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:21,735 : INFO : PROGRESS: at 64.67% examples, 431466 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:22,762 : INFO : PROGRESS: at 65.60% examples, 431599 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:23,763 : INFO : PROGRESS: at 66.51% examples, 431641 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:24,765 : INFO : PROGRESS: at 67.30% examples, 431909 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:25,773 : INFO : PROGRESS: at 68.28% examples, 431901 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:26,778 : INFO : PROGRESS: at 69.12% examples, 431775 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:27,806 : INFO : PROGRESS: at 70.04% examples, 431534 words/s, in_qsize 15, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-01-08 06:17:28,838 : INFO : PROGRESS: at 70.93% examples, 431666 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:29,857 : INFO : PROGRESS: at 71.84% examples, 431469 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:30,880 : INFO : PROGRESS: at 72.75% examples, 431974 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:31,891 : INFO : PROGRESS: at 73.65% examples, 431967 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:32,899 : INFO : PROGRESS: at 74.56% examples, 431817 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:33,901 : INFO : PROGRESS: at 75.41% examples, 431834 words/s, in_qsize 15, out_qsize 1\n",
      "2018-01-08 06:17:34,900 : INFO : PROGRESS: at 76.31% examples, 431750 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:35,925 : INFO : PROGRESS: at 77.14% examples, 431873 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:36,943 : INFO : PROGRESS: at 78.11% examples, 432043 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:37,951 : INFO : PROGRESS: at 79.00% examples, 431920 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:38,974 : INFO : PROGRESS: at 79.94% examples, 432049 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:39,989 : INFO : PROGRESS: at 80.87% examples, 432223 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:41,037 : INFO : PROGRESS: at 81.77% examples, 432031 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:42,040 : INFO : PROGRESS: at 82.64% examples, 432354 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:43,042 : INFO : PROGRESS: at 83.56% examples, 432488 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:44,087 : INFO : PROGRESS: at 84.50% examples, 432288 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:45,112 : INFO : PROGRESS: at 85.44% examples, 432597 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:46,136 : INFO : PROGRESS: at 86.29% examples, 432209 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:47,140 : INFO : PROGRESS: at 87.18% examples, 432699 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:48,143 : INFO : PROGRESS: at 88.09% examples, 432615 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:49,216 : INFO : PROGRESS: at 89.04% examples, 432616 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:50,245 : INFO : PROGRESS: at 90.04% examples, 432792 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:51,261 : INFO : PROGRESS: at 90.93% examples, 432939 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:52,267 : INFO : PROGRESS: at 91.89% examples, 433028 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:53,294 : INFO : PROGRESS: at 92.72% examples, 433111 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:54,317 : INFO : PROGRESS: at 93.65% examples, 433132 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:55,339 : INFO : PROGRESS: at 94.63% examples, 433319 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:56,365 : INFO : PROGRESS: at 95.55% examples, 433401 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:57,379 : INFO : PROGRESS: at 96.46% examples, 433360 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:17:58,388 : INFO : PROGRESS: at 97.32% examples, 433770 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:17:59,389 : INFO : PROGRESS: at 98.27% examples, 433688 words/s, in_qsize 16, out_qsize 0\n",
      "2018-01-08 06:18:00,454 : INFO : PROGRESS: at 99.21% examples, 433702 words/s, in_qsize 15, out_qsize 0\n",
      "2018-01-08 06:18:01,111 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2018-01-08 06:18:01,124 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2018-01-08 06:18:01,145 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2018-01-08 06:18:01,154 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2018-01-08 06:18:01,166 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-01-08 06:18:01,181 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-01-08 06:18:01,192 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-01-08 06:18:01,196 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-01-08 06:18:01,197 : INFO : training on 49298580 raw words (49059199 effective words) took 113.0s, 434091 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "49059199"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train this model\n",
    "# model.train(docs, \n",
    "#             total_examples=len(docs), \n",
    "#             epochs=20)\n",
    "model.train(labeled_docs, \n",
    "            total_examples=len(labeled_docs),\n",
    "            epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docvec2: [-0.07173967 -0.00501047  0.07555088  0.16436286 -0.36748344 -0.01681295\n",
      "  0.02698558 -0.18099998  0.23837043  0.08042304 -0.01240299  0.24280906\n",
      "  0.18474372  0.09306788 -0.28878078  0.06376255  0.08662205  0.16764431\n",
      "  0.11876337  0.15184912]\n",
      "docvecsyn2: [-0.07173967 -0.00501047  0.07555088  0.16436286 -0.36748344 -0.01681295\n",
      "  0.02698558 -0.18099998  0.23837043  0.08042304 -0.01240299  0.24280906\n",
      "  0.18474372  0.09306788 -0.28878078  0.06376255  0.08662205  0.16764431\n",
      "  0.11876337  0.15184912]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('SENT_17542', 0.9056956768035889),\n",
       " ('SENT_2676', 0.8908694982528687),\n",
       " ('SENT_21091', 0.8898348808288574),\n",
       " ('SENT_18791', 0.8878933191299438),\n",
       " ('SENT_13271', 0.8822087049484253),\n",
       " ('SENT_14911', 0.881842851638794),\n",
       " ('SENT_22648', 0.8797429800033569),\n",
       " ('SENT_11599', 0.877663254737854),\n",
       " ('SENT_21024', 0.8768661022186279),\n",
       " ('SENT_21117', 0.8742849230766296)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the trained document vector\n",
    "docvec2 = model.docvecs[0]\n",
    "docvecsyn2 = model.docvecs.doctag_syn0[0]\n",
    "\n",
    "# calculate most similar documents\n",
    "# (we expect the results to be correct)\n",
    "docsim2 = model.docvecs.most_similar(id_str)\n",
    "\n",
    "## THIS NOW SHOWS THE BETTER RESULTS AS THE MODEL IS NOW TRAINED\n",
    "print 'docvec2: %s' % docvec2[:20]\n",
    "print 'docvecsyn2: %s' % docvecsyn2[:20]\n",
    "docsim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document vector:\n",
      "[-0.07173967 -0.00501047  0.07555088  0.16436286 -0.36748344]\n",
      "[-0.07173967 -0.00501047  0.07555088  0.16436286 -0.36748344]\n",
      "[-0.07173967 -0.00501047  0.07555088  0.16436286 -0.36748344]\n",
      "[-0.07173967 -0.00501047  0.07555088  0.16436286 -0.36748344]\n",
      "\n",
      "Most similar:\n",
      "[('SENT_22468', 0.2260284423828125), ('SENT_16903', 0.21625030040740967)]\n",
      "[('SENT_17542', 0.9056956768035889), ('SENT_2676', 0.8908694982528687)]\n",
      "\n",
      "Infered vector:\n",
      "[ 0.03901744  0.04886983 -0.03233307  0.06236155 -0.16637127]\n",
      "[-0.01654357  0.08957674  0.05339311  0.14227058 -0.30839854]\n",
      "[-0.23123467  0.13793825  0.32103819  0.19679461 -0.34883693]\n",
      "[-0.43890652  0.61093193  0.54345572  0.24288487 -0.09800234]\n",
      "\n",
      "Norm of infered vector:\n",
      "1.48904\n",
      "2.93619\n",
      "5.82037\n",
      "10.7574\n"
     ]
    }
   ],
   "source": [
    "# choose one document\n",
    "doc = labeled_docs[0].words\n",
    "\n",
    "# infer vectors with different 'steps' parameters\n",
    "infervec1 = model.infer_vector(doc, alpha=0.025, min_alpha=0.01, steps=1)\n",
    "infervec2 = model.infer_vector(doc, alpha=0.025, min_alpha=0.01, steps=10)\n",
    "infervec3 = model.infer_vector(doc, alpha=0.025, min_alpha=0.01, steps=100)\n",
    "infervec4 = model.infer_vector(doc, alpha=0.025, min_alpha=0.01, steps=1000)\n",
    "\n",
    "# print results\n",
    "\n",
    "# document vector\n",
    "print('\\nDocument vector:')\n",
    "\n",
    "# we can see that the document vectors do not change after training.\n",
    "print(docvec1[:5])\n",
    "print(docvec2[:5])\n",
    "print(docvecsyn1[:5])\n",
    "print(docvecsyn2[:5])\n",
    "\n",
    "# most similar documents\n",
    "print('\\nMost similar:')\n",
    "\n",
    "# before training, the result is wrong. after training, correct. good.\n",
    "print(docsim1[:2])\n",
    "print(docsim2[:2])\n",
    "\n",
    "# infered vectors with different 'steps' parameters\n",
    "print('\\nInfered vector:')\n",
    "\n",
    "# we can see that, they are quite different.\n",
    "print(infervec1[:5])\n",
    "print(infervec2[:5])\n",
    "print(infervec3[:5])\n",
    "print(infervec4[:5])\n",
    "\n",
    "# norm of inferred vectors\n",
    "print('\\nNorm of infered vector:')\n",
    "\n",
    "# it seems that, the norm of inferred vectors would be larger for bigger steps\n",
    "print(np.linalg.norm(infervec1))\n",
    "print(np.linalg.norm(infervec2))\n",
    "print(np.linalg.norm(infervec3))\n",
    "print(np.linalg.norm(infervec4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SENT_17542', 0.9056956768035889),\n",
       " ('SENT_2676', 0.8908694982528687),\n",
       " ('SENT_21091', 0.8898348808288574),\n",
       " ('SENT_18791', 0.8878933191299438),\n",
       " ('SENT_13271', 0.8822087049484253),\n",
       " ('SENT_14911', 0.881842851638794),\n",
       " ('SENT_22648', 0.8797429800033569),\n",
       " ('SENT_11599', 0.877663254737854),\n",
       " ('SENT_21024', 0.8768661022186279),\n",
       " ('SENT_21117', 0.8742849230766296)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = labeled_docs[0]\n",
    "docsim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Le Marais Bakery is expanding to Ghirardelli Square, and is seeking motivated, enthusiastic baristas to join their team. We offer training and support from Stumptown Coffee Roasters. Part-time and full-time positions available.\\n\\nCandidates should have a genuine sense of hospitality, and a love and knowledge of baked items and coffee. All positions offer amazing tips and the opportunity to work with a great team, along with appreciative customers and a welcoming atmosphere.\\n\\nJob Requirements:\\nExcel in customer service\\nThe ability to thrive in a fast paced environment\\nExisting CA Food Handler's Certification or certification within 30 days post-hire\\nWork well in a team setting\\n\\nWith our upcoming expansion to Ghirardelli Square, there are many opportunities for growth and advancement.\\n\""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Line cook/prep'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[21024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hiring experienced server, line cook, server, and dishwasher'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jobs.iloc[21117].description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
